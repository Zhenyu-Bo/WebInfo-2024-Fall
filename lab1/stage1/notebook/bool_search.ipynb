{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import jieba\n",
    "import pkuseg\n",
    "import ast\n",
    "\n",
    "def is_word_valid(word):\n",
    "    \"\"\"\n",
    "    检查词语是否包含非法字符。\n",
    "    \n",
    "    参数：\n",
    "    - word: 要检查的词语。\n",
    "    \n",
    "    返回：\n",
    "    - 如果词语不包含非法字符，则返回 True；否则返回 False。\n",
    "    \"\"\"\n",
    "    # 方法1：使用定义的非法字符集\n",
    "    for char in word:\n",
    "        if char in ILLEGAL_CHARS:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "    # 方法2：使用正则表达式\n",
    "    # return not bool(ILLEGAL_CHARS_PATTERN.search(word))\n",
    "\n",
    "def escape_illegal_chars(text):\n",
    "    \"\"\"\n",
    "    将非法字符转换为 Unicode 转义序列。\n",
    "    \n",
    "    参数：\n",
    "    - text: 原始字符串。\n",
    "    \n",
    "    返回：\n",
    "    - 转义后的字符串。\n",
    "    \"\"\"\n",
    "    return text.encode('unicode_escape').decode('utf-8')\n",
    "\n",
    "# 定义非法字符集\n",
    "ILLEGAL_CHARS = {'§', '#', '$', '%', '&', '*', '!', '@', '^', '(', ')', '-', '=', '+', '{', '}', '[', ']', '|', '\\\\', ':', ';', '\"', \"'\", '<', '>', ',', '.', '?', '/'}\n",
    "\n",
    "# 或者使用正则表达式排除含有非法字符的词语\n",
    "# ILLEGAL_CHARS_PATTERN = re.compile(r'[§#$%&*!@^()\\-=+{}\\[\\]|\\\\:;\"\\'<>,.?/]')\n",
    "\n",
    "# 选择分词工具\n",
    "tool_choice = input(\"请选择分词工具（输入 'jieba' 或 'pkuseg'）：\")\n",
    "if tool_choice.lower() == 'jieba':\n",
    "    use_jieba = True\n",
    "    print(\"使用 Jieba 分词\")\n",
    "elif tool_choice.lower() == 'pkuseg':\n",
    "    use_jieba = False\n",
    "    print(\"使用 PKUSeg 分词\")\n",
    "else:\n",
    "    print(\"输入有误，默认使用 Jieba 分词\")\n",
    "    use_jieba = True\n",
    "\n",
    "# 加载停用词表\n",
    "stopwords_file = 'data/cn_stopwords.txt'\n",
    "stopwords = set()\n",
    "with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        if word:\n",
    "            stopwords.add(word)\n",
    "\n",
    "# 加载同义词词典\n",
    "synonyms_file = 'data/syno_from_baidu_hanyu.txt'\n",
    "synonym_dict = {}\n",
    "with open(synonyms_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        words = line.strip().split()\n",
    "        if words:\n",
    "            representative = words[0]\n",
    "            for word in words:\n",
    "                synonym_dict[word] = representative\n",
    "\n",
    "# 定义文件列表\n",
    "file_list = [\n",
    "    {'input': '../data/selected_book_top_1200_data_tag.csv', 'output': 'data/book_output.csv'},\n",
    "    {'input': '../data/selected_movie_top_1200_data_tag.csv', 'output': 'data/movie_output.csv'}\n",
    "]\n",
    "\n",
    "# 初始化 pkuseg 分词器\n",
    "if not use_jieba:\n",
    "    seg = pkuseg.pkuseg()\n",
    "\n",
    "for file_pair in file_list:\n",
    "    input_file = file_pair['input']\n",
    "    output_file = file_pair['output']\n",
    "    print(f\"正在处理文件：{input_file}\")\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as csvfile_in, \\\n",
    "         open(output_file, 'w', encoding='utf-8', newline='') as csvfile_out:\n",
    "        reader = csv.reader(csvfile_in)\n",
    "        writer = csv.writer(csvfile_out, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        # 跳过输入文件的第一行\n",
    "        next(reader)\n",
    "\n",
    "        # 写入输出文件的表头\n",
    "        header = ['id', 'words']\n",
    "        writer.writerow(header)\n",
    "\n",
    "        for row in reader:\n",
    "            item_id = row[0]\n",
    "            tags_str = row[1]\n",
    "\n",
    "            # 解析 Tags 字段\n",
    "            try:\n",
    "                tags_set = ast.literal_eval(tags_str)\n",
    "            except Exception as e:\n",
    "                print(f\"解析文件 {input_file} 中的 ID {item_id} 的标签时出错: {e}\")\n",
    "                continue\n",
    "\n",
    "            # 分词、去停用词、替换同义词\n",
    "            new_tags = set()\n",
    "            for tag in tags_set:\n",
    "                if use_jieba:\n",
    "                    words = jieba.lcut(tag)\n",
    "                else:\n",
    "                    words = seg.cut(tag)\n",
    "\n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    if word and word not in stopwords and is_word_valid(word):\n",
    "                        # 替换同义词\n",
    "                        representative = synonym_dict.get(word, word)\n",
    "                        # 转义单引号\n",
    "                        representative = representative.replace(\"'\", \"\\\\'\")\n",
    "                        # 转义非法字符（此步骤可选，如果已经过滤，可以移除）\n",
    "                        # representative = escape_illegal_chars(representative)\n",
    "                        new_tags.add(representative)\n",
    "\n",
    "            # 重建 Tags 字符串，确保格式正确\n",
    "            new_tags_str = \"{\" + \", \".join(f\"'{tag}'\" for tag in new_tags) + \"}\"\n",
    "            writer.writerow([item_id, new_tags_str])\n",
    "\n",
    "    print(f\"文件 {input_file} 处理完成，结果已保存到 {output_file}\")\n",
    "\n",
    "print(\"所有文件处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成倒排索引表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author: Zhenyu Bo\n",
    "# Date: 2024-11-06\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def read_words_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    读取 CSV 文件，提取所有单词和文档内容。\n",
    "\n",
    "    参数：\n",
    "    - file_path: CSV 文件路径。\n",
    "\n",
    "    返回：\n",
    "    - all_words: 所有词项的集合。\n",
    "    - documents: 字典，键为文档 ID，值为该文档包含的单词集合。\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path, dtype={'id': int, 'words': str})\n",
    "    all_words = set()\n",
    "    documents = {}\n",
    "    for idx in range(len(data)):\n",
    "        words = ast.literal_eval(data.at[idx, 'words'])\n",
    "        doc_id = data.at[idx, 'id']\n",
    "        documents[doc_id] = words\n",
    "        all_words.update(words)\n",
    "    return all_words, documents\n",
    "\n",
    "\n",
    "def generate_inverted_index_table(all_words, documents, output_file):\n",
    "    \"\"\"\n",
    "    生成倒排索引表并保存为 CSV 文件。\n",
    "\n",
    "    参数：\n",
    "    - all_words: 集合，包含所有词项。\n",
    "    - documents: 字典，键为文档 ID，值为该文档的标签集合。\n",
    "    - output_file: 输出的 CSV 文件路径。\n",
    "    \"\"\"\n",
    "    inverted_index_table = []\n",
    "    for word in all_words:\n",
    "        doc_ids = [doc_id for doc_id, words in documents.items() if word in words]\n",
    "        doc_ids_sorted = sorted(doc_ids)\n",
    "        num_docs = len(doc_ids_sorted)\n",
    "        l = int(sqrt(num_docs))  # 设置跳表的间隔\n",
    "        skip_table = []\n",
    "        if num_docs > l:\n",
    "            for i in range(num_docs):\n",
    "                if i % l == 0:\n",
    "                    if i < num_docs - l:\n",
    "                        skip_info = {'index': i + l, 'value': doc_ids_sorted[i + l]}\n",
    "                    else:\n",
    "                        # 最后一个跳表指针指向末尾\n",
    "                        skip_info = {'index': num_docs - 1, 'value': doc_ids_sorted[num_docs - 1]}\n",
    "                    skip_table.append(skip_info)\n",
    "        else:\n",
    "            skip_info = {'index': None, 'value': None}\n",
    "            skip_table.append(skip_info)\n",
    "        inverted_index_table.append({'word': word, 'id_list': doc_ids_sorted, 'skip_table': skip_table})\n",
    "    pd.DataFrame(inverted_index_table).to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "# 处理书籍数据\n",
    "all_book_words, book_documents = read_words_from_csv(\"../data/book_output.csv\")\n",
    "generate_inverted_index_table(all_book_words, book_documents, \"../data/book_inverted_index_table.csv\")\n",
    "print(\"书籍倒排索引表已成功生成\")\n",
    "\n",
    "# 处理电影数据\n",
    "all_movie_words, movie_documents = read_words_from_csv(\"../data/movie_output.csv\")\n",
    "generate_inverted_index_table(all_movie_words, movie_documents, \"../data/movie_inverted_index_table.csv\")\n",
    "print(\"电影倒排索引表已成功生成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取id序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author: Zhenyu Bo\n",
    "# Date: 2024-11-14\n",
    "\n",
    "\"\"\"\n",
    "从数据集中提取 ID 列，保存到文本文件中。\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def extract_ids(input_csv, output_txt):\n",
    "    # 读取 CSV 文件\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # 提取 ID 列（假设 ID 在第一列）\n",
    "    ids = df.iloc[:, 0]\n",
    "\n",
    "    # 将 ID 写入文本文件\n",
    "    with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "        for id in ids:\n",
    "            f.write(f\"{id}\\n\")\n",
    "\n",
    "\n",
    "# 处理书籍 ID\n",
    "extract_ids('../../data/selected_book_top_1200_data_tag.csv', '../data/Book_id.txt')\n",
    "print('Book_id.txt 文件已生成。')\n",
    "\n",
    "# 处理电影 ID\n",
    "extract_ids('../../data/selected_movie_top_1200_data_tag.csv', '../data/Movie_id.txt')\n",
    "print('Movie_id.txt 文件已生成。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行布尔查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# 倒排索引表，movie_inverted_index_table.csv \n",
    "# 全id表，Movie_id.txt\n",
    "# 词表，movie_words.csv 用于打印结果 目前格式同助教提供的selected_book_top_1200.csv\n",
    "\n",
    "\n",
    "def read_inverted_index(file_path):\n",
    "    \"\"\"\n",
    "    读取倒排索引表，返回字典格式的数据。\n",
    "    \"\"\"\n",
    "    inverted_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            word = row['word']\n",
    "            id_list = ast.literal_eval(row['id_list'])\n",
    "            # skip_table 不在此处使用，可省略或保留\n",
    "            inverted_index[word] = id_list\n",
    "    return inverted_index\n",
    "\n",
    "def read_all_ids(file_path):\n",
    "    \"\"\"\n",
    "    读取所有ID，返回ID的集合。\n",
    "    \"\"\"\n",
    "    all_ids = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            id = int(line.strip())\n",
    "            all_ids.add(id)\n",
    "    return all_ids\n",
    "\n",
    "def tokenize(expression):\n",
    "    \"\"\"\n",
    "    将布尔表达式分词。\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r'AND|OR|NOT|\\w+|\\(|\\)', expression)\n",
    "    return tokens\n",
    "\n",
    "def infix_to_postfix(tokens):\n",
    "    \"\"\"\n",
    "    将中缀表达式转换为后缀表达式（逆波兰表达式）。\n",
    "    \"\"\"\n",
    "    precedence = {'NOT': 3, 'AND': 2, 'OR': 1}\n",
    "    output = []\n",
    "    operator_stack = []\n",
    "    operators = {'AND', 'OR', 'NOT'}\n",
    "    for token in tokens:\n",
    "        if token not in operators and token not in {'(', ')'}:\n",
    "            output.append(token)\n",
    "        elif token == '(':\n",
    "            operator_stack.append(token)\n",
    "        elif token == ')':\n",
    "            while operator_stack and operator_stack[-1] != '(':\n",
    "                output.append(operator_stack.pop())\n",
    "            operator_stack.pop()  # 弹出 '('\n",
    "        else:  # 操作符\n",
    "            while operator_stack and operator_stack[-1] != '(' and precedence.get(operator_stack[-1], 0) >= precedence.get(token, 0):\n",
    "                output.append(operator_stack.pop())\n",
    "            operator_stack.append(token)\n",
    "    while operator_stack:\n",
    "        output.append(operator_stack.pop())\n",
    "    return output\n",
    "\n",
    "def evaluate_postfix(postfix_tokens, inverted_index, all_ids):\n",
    "    \"\"\"\n",
    "    评估后缀表达式，返回符合条件的ID集合。\n",
    "    \"\"\"\n",
    "    stack = []\n",
    "    operators = {'AND', 'OR', 'NOT'}\n",
    "    for token in postfix_tokens:\n",
    "        if token not in operators:\n",
    "            if token in inverted_index:\n",
    "                stack.append(set(inverted_index[token]))\n",
    "            else:\n",
    "                stack.append(set())\n",
    "        elif token == 'NOT':\n",
    "            operand = stack.pop()\n",
    "            stack.append(all_ids - operand)\n",
    "        else:\n",
    "            right = stack.pop()\n",
    "            left = stack.pop()\n",
    "            if token == 'AND':\n",
    "                stack.append(left & right)\n",
    "            elif token == 'OR':\n",
    "                stack.append(left | right)\n",
    "    return stack.pop() if stack else set()\n",
    "\n",
    "def display_results(result_ids, words_df):\n",
    "    \"\"\"\n",
    "    根据查询结果的ID，从 DataFrame 中查找并打印ID和标签。\n",
    "    \"\"\"\n",
    "    df = words_df[words_df['id'].isin(result_ids)]\n",
    "    if not df.empty:\n",
    "        for index, row in df.iterrows():\n",
    "            id = row['id']\n",
    "            words = ast.literal_eval(row['words'])\n",
    "            print(f\"ID: {id}\")\n",
    "            print(f\"标签: {', '.join(words)}\\n\")\n",
    "    else:\n",
    "        print(\"没有符合条件的结果。\")\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# 主程序\n",
    "print(\"正在加载数据，请稍候...\")\n",
    "\n",
    "# 加载书籍数据\n",
    "book_inverted_index = read_inverted_index('../data/book_inverted_index_table.csv')\n",
    "book_all_ids = read_all_ids('../data/Book_id.txt')\n",
    "book_words_df = pd.read_csv('../data/book_words.csv', dtype={'id': int, 'words': str})\n",
    "\n",
    "# 加载电影数据\n",
    "movie_inverted_index = read_inverted_index('../data/movie_inverted_index_table.csv')\n",
    "movie_all_ids = read_all_ids('../data/Movie_id.txt')\n",
    "movie_words_df = pd.read_csv('../data/movie_words.csv', dtype={'id': int, 'words': str})\n",
    "\n",
    "print(\"数据加载完成！\")\n",
    "\n",
    "while True:\n",
    "    choice = input(\"请选择查询类型（1 - 书籍，2 - 电影）：\\n\")\n",
    "    if choice == '1':\n",
    "        inverted_index = book_inverted_index\n",
    "        all_ids = book_all_ids\n",
    "        words_df = book_words_df\n",
    "    elif choice == '2':\n",
    "        inverted_index = movie_inverted_index\n",
    "        all_ids = movie_all_ids\n",
    "        words_df = movie_words_df\n",
    "    else:\n",
    "        print(\"输入错误，请输入 1 或 2。\")\n",
    "        continue  # 重新开始循环\n",
    "\n",
    "    expression = input(\"请输入布尔查询表达式：\\n\")\n",
    "    # 记录查询开始时间\n",
    "    start_time = time.time()\n",
    "    tokens = tokenize(expression)\n",
    "    postfix_tokens = infix_to_postfix(tokens)\n",
    "    result_ids = evaluate_postfix(postfix_tokens, inverted_index, all_ids)\n",
    "    # 计算查询时间\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if result_ids:\n",
    "        print(\"查询结果：\\n\")\n",
    "        display_results(result_ids, words_df)\n",
    "    else:\n",
    "        print(\"没有符合条件的结果。\")\n",
    "    \n",
    "    print(f\"查询耗时：{elapsed_time:.6f} 秒\\n\")\n",
    "\n",
    "    cont = input(\"是否继续查询？(Y/N): \")\n",
    "    if cont.strip().lower() != 'y':\n",
    "        print(\"感谢您的使用！\")\n",
    "        break  # 退出循环，结束程序\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
