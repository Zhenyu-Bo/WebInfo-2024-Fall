# 分词说明
两个文件，`split_words.py`和`split_words_pro.py`
均由AI直接生成

- `split_words.py`
  - 输入：`lab1\data\selected_book_top_1200_data_tag.csv`和`lab1\data\selected_movie_top_1200_data_tag.csv`
  - 输出：`split_output/book_words.csv`和`split_output/movie_words.csv`
  - 功能：使用jieba或thulac或both（可以在代码中指定）进行分词
- `split_words_pro.py`
  - 输入：
    - `lab1\data\selected_book_top_1200_data_tag.csv`
    - `lab1\data\selected_movie_top_1200_data_tag.csv`
    - `lab1\stage1\data\baidu_stopwords.txt` 百度停用词表
    - `lab1\stage1\data\dict_synonym.txt` 同义词表
  - 输出：`split_output/pro/book_words.csv`和`split_output/pro/movie_words.csv`。与`split_words.py`输出到不同目录，便于比较然后写报告
  - 功能：添加了基于停用词的筛选和基于同义词的合并

# 存在问题
  - pro对`selected_movie_top_1200_data_tag.csv`执行速度很慢（在我电脑上要20分钟）